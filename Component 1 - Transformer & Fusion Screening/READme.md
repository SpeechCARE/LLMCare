# Component 1 â€“ Transformer & Fusion Screening

## Overview

This component develops a **screening algorithm** for detecting cognitive decline using both **transformer-based embeddings** and **handcrafted linguistic features**, combined through a **fusion classifier**.

* **Transformer Baselines**: Evaluated 10 pretrained models (general-purpose and biomedical/clinical). Compared frozen, last-layer, and full fine-tuning strategies.
* **Handcrafted Features**: 110 lexical, syntactic, semantic, and psycholinguistic features for interpretability.
* **Fusion Classifier**: Combines transformer embeddings with handcrafted features via a late fusion strategy for improved accuracy and robustness.

## Project Structure

```
â”œâ”€â”€ Config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ config.py            # Hyperparameters (batch size, learning rate, etc.)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ dataset.py           # Dataset loading and preprocessing
â”‚
â”œâ”€â”€ Model/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ model.py             # Transformer, MLP, and fusion model definitions
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ global_constants.py  # Data paths and directory constants
â”‚   â”œâ”€â”€ supplementary.py     # Extra utilities (e.g., metrics, feature extraction)
â”‚   â””â”€â”€ utils.py             # Helper functions (logging, checkpointing, etc.)
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ extract_handcrafted_features.ipynb  # Run this before main.py
â”‚
â”œâ”€â”€ main.py                  # Entry point script
â”œâ”€â”€ train.py                 # Training loop
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ README.md
```

## Installation

Install dependencies:

```bash
pip install -r requirements.txt
```

## Running

### Step 1 â€“ Extract Handcrafted Features

Before training or running the pipeline, you must generate the handcrafted linguistic features.
Open and run the notebook:

```bash
notebooks/extract_handcrafted_features.ipynb
```

This will produce a feature file (e.g., features.csv) that is later combined with transformer embeddings.

### Step 2 â€“ Train / Run Pipeline

Run the main script:

```bash
python main.py
```

---

## ðŸ”§ Configuration

You can customize the experiment in two places:

### 1. **Config/config.py** â€“ Hyperparameters

Here you control model and training settings.

**Example snippet:**

```python
# Config/config.py
BATCH_SIZE = 8
LEARNING_RATE = 2e-5
EPOCHS = 50
DROPOUT = 0.4
MODEL_NAME = "bert-base-uncased"
```

* Change `MODEL_NAME` to try different transformers (`roberta-base`, `distilbert-base-uncased`, etc.).
* Increase/decrease `BATCH_SIZE` depending on GPU memory.
* Adjust `LEARNING_RATE` or `EPOCHS` to balance training stability vs. overfitting.

---

### 2. **utils/global_constants.py** â€“ Data Paths & Globals

This file stores dataset paths and directory constants.

**Example snippet:**

```python
# utils/global_constants.py
TRAIN_DATA_PATH = "./data/train.csv"
VAL_DATA_PATH   = "./data/val.csv"
TEST_DATA_PATH  = "./data/test.csv"

CHECKPOINT_DIR = "./checkpoints/"
LOG_DIR        = "./logs/"
```

* Update these paths if your dataset is stored elsewhere.
* Change `CHECKPOINT_DIR` to decide where trained models are saved.
* Set `LOG_DIR` for TensorBoard or training logs.
