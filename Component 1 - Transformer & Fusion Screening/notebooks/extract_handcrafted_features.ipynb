{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC0FjjYroXTD",
        "outputId": "96070a89-a9f0-40df-b4bd-3e1e59a7eab2"
      },
      "outputs": [],
      "source": [
        "!pip install -q deplacy\n",
        "!pip install -q lexicalrichness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfsUJNif2NdK",
        "outputId": "fd18a30b-d72e-46a6-d3eb-d14c0245e34b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import glob\n",
        "import librosa\n",
        "import os\n",
        "import string\n",
        "from copy import deepcopy\n",
        "from sklearn.feature_extraction.text import CountVectorizer #convert text comment into a numeric vector\n",
        "from sklearn.feature_extraction.text import TfidfTransformer #use TF IDF transformer to change text vector created by count vectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.svm import SVC# Support Vector Machine\n",
        "from sklearn.pipeline import Pipeline #pipeline to implement steps in series\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.ensemble import *\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import ElasticNetCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# import preprocessor as p\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=2021)\n",
        "# import gensim\n",
        "from sklearn.model_selection import train_test_split\n",
        "import textblob\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "import re\n",
        "import spacy\n",
        "# specify GPU\n",
        "import torch\n",
        "device = torch.device(\"cuda\")\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from lexicalrichness import LexicalRichness\n",
        "from collections import Counter\n",
        "from string import punctuation\n",
        "import math\n",
        "import deplacy\n",
        "from copy import deepcopy\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKd-Tj3hOMsZ"
      },
      "source": [
        "# Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taX9EPbD-2DF"
      },
      "outputs": [],
      "source": [
        "path = '/workspace/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbI3O4MPeutk"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_rows', 700)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xtppkNmwWBE"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = path + 'Data/'\n",
        "\n",
        "FILE_PATH = DATA_PATH + './train_df_story_recall_cinderella.csv' # 4-1 were tested.\n",
        "\n",
        "class_label = [\"Control_group\", \"Ad_Alzhiemer\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H6p38DFGQLU"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(FILE_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6z8cn0EY8hzR"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2SKf7HH6fs4",
        "outputId": "dd2537da-f8fc-4efb-a0eb-97162d899442"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(124, 10)"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if ('transcription' in df.columns) and ('text' not in df.columns):\n",
        "    df = df.rename(columns={'transcription':'text'})\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYjSY-NDFngu"
      },
      "outputs": [],
      "source": [
        "def preprocesssing(df_text):\n",
        "    # remove punctuation marks\n",
        "    df_text = df_text.rename(columns={'Text': 'text'})\n",
        "    df_text['clean_text'] = df_text['text'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
        "\n",
        "    punctuation = '!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~'\n",
        "\n",
        "    df_text['clean_text'] = df_text['clean_text'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))\n",
        "    # test['clean_tweet'] = test['clean_tweet'].apply(lambda x: ''.join(ch for ch in x if ch not in set(punctuation)))\n",
        "\n",
        "    # convert text to lowercase\n",
        "    df_text['clean_text'] = df_text['clean_text'].str.lower()\n",
        "    # test['clean_tweet'] = test['clean_tweet'].str.lower()\n",
        "\n",
        "    # remove numbers\n",
        "    df_text['clean_text'] = df_text['clean_text'].str.replace(\"[0-9]\", \" \")\n",
        "    # test['clean_tweet'] = test['clean_tweet'].str.replace(\"[0-9]\", \" \")\n",
        "\n",
        "    # remove whitespaces\n",
        "    df_text['clean_text'] = df_text['clean_text'].apply(lambda x:' '.join(x.split()))\n",
        "    # test['clean_tweet'] = test['clean_tweet'].apply(lambda x: ' '.join(x.split()))\n",
        "    return df_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAmuCy9-z0vF"
      },
      "outputs": [],
      "source": [
        "df_text = df.copy()\n",
        "df_text = preprocesssing(deepcopy(df_text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4IZ1jN2FLzA"
      },
      "source": [
        "# POS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "m785J4NdHAwR",
        "outputId": "04c4d795-cf4a-439e-9c3b-7d38cce3e4dc"
      },
      "outputs": [],
      "source": [
        "# load en_core_web_sm of English for vocabluary, syntax & entities\n",
        "nlp = en_core_web_sm.load()\n",
        "#  \"nlp\" Objectis used to create documents with linguistic annotations.\n",
        "df_text[\"POS\"] = df_text[\"clean_text\"].apply(lambda x:\" \".join([word.pos_ for word in nlp(x)]))\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_v = vectorizer.fit_transform(df_text[\"POS\"].tolist())\n",
        "df_pos = pd.DataFrame(data=X_v.toarray(), columns = [pos.upper() for pos in vectorizer.get_feature_names_out()])\n",
        "\n",
        "df_text_pos = pd.concat([df_text, df_pos], axis=1)\n",
        "df_text_pos.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wF_8ICMyFPY9"
      },
      "source": [
        "# TAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "nDNJcrLR-HPB",
        "outputId": "fcf5becb-70bb-4d4d-9e8d-e5091c48f378"
      },
      "outputs": [],
      "source": [
        "df_text_pos[\"TAG\"] = df_text[\"clean_text\"].apply(lambda x:\" \".join([word.tag_ for word in nlp(x)]))\n",
        "\n",
        "vectorizer1 = CountVectorizer()\n",
        "X_tag = vectorizer1.fit_transform(df_text_pos[\"TAG\"].tolist())\n",
        "df_tag = pd.DataFrame(data=X_tag.toarray(), columns = [tag.upper() for tag in vectorizer1.get_feature_names_out()])\n",
        "\n",
        "df_t_p_t = pd.concat([df_text_pos, df_tag], axis=1)\n",
        "df_t_p_t.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVEIV0UE5-E_"
      },
      "source": [
        "# Some of Linguistic Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNWqC-eCYWDL"
      },
      "source": [
        "## Content Density"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLUgK8D_yXaB",
        "outputId": "70772ed6-3003-4bc1-f6dc-e641a227df98"
      },
      "outputs": [],
      "source": [
        "df_t_p_t[\"Content Density\"] = (df_t_p_t[\"ADJ\"] + df_t_p_t[\"ADV\"] + df_t_p_t[\"VERB\"] + df_t_p_t[\"NOUN\"]) / df_t_p_t.loc[:, \"ADJ\":\"VERB\"].drop([\"ADJ\", \"ADV\", \"VERB\", \"NOUN\", \"PUNCT\"], axis=1).sum(axis=1)\n",
        "df_t_p_t.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcQTLGePYgXc"
      },
      "source": [
        "## Part-of-Speech rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XFraHiE4yXAW",
        "outputId": "a80a5d9b-2359-41cd-c413-bbd8efa82d8b"
      },
      "outputs": [],
      "source": [
        "pos = [\"ADJ\", \"ADV\", \"DET\", \"CONJ\", \"INTJ\", \"NOUN\", \"NUM\", \"IN\", \"PRON\", \"VERB\"]\n",
        "sum_pos = []\n",
        "for i in range(len(pos)):\n",
        "    if pos[i] == \"CONJ\":\n",
        "        Seri_CONJ = df_t_p_t[\"CCONJ\"] + df_t_p_t[\"SCONJ\"]\n",
        "        sum_pos.append(Seri_CONJ / Seri_CONJ.sum())\n",
        "    else:\n",
        "        sum_pos.append(df_t_p_t[pos[i]] / df_t_p_t[pos[i]].sum())\n",
        "\n",
        "df_t_p_t[\"Part-of-Speech rate\"] = sum(sum_pos) / 10\n",
        "df_t_p_t.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-mjz6uoYnKe"
      },
      "source": [
        "## Reference Rate to Reality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkxqHkuYyW9P",
        "outputId": "db07a16a-9928-4e65-b5b5-377918fc0acb"
      },
      "outputs": [],
      "source": [
        "df_t_p_t[\"Reference Rate to Reality\"] = df_t_p_t[\"NOUN\"] / df_t_p_t[\"VERB\"]\n",
        "df_t_p_t.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2_zMp7pa0fL"
      },
      "source": [
        "## Relative pronouns rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5_2KU-_yW0H"
      },
      "outputs": [],
      "source": [
        "df_t_p_t[\"Relative pronouns rate\"] = (df_t_p_t[\"WDT\"] + df_t_p_t[\"WP\"]) / (df_t_p_t[\"WDT\"] + df_t_p_t[\"WP\"]).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKksioLNDfW2"
      },
      "source": [
        "## Negative adverbs rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHNAXchlBhEK"
      },
      "outputs": [],
      "source": [
        "def negative_adverb_count(x):\n",
        "    cnt = 0\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    for word in nlp(x):\n",
        "        if word.pos_ == \"ADV\":\n",
        "            if sid.polarity_scores(str(word))['neg'] > sid.polarity_scores(str(word))['pos']:\n",
        "                cnt += 1\n",
        "    return cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQGff9vv7kL_",
        "outputId": "8ddfac62-d76e-4fed-b11f-0fbf32ab8b50"
      },
      "outputs": [],
      "source": [
        "Seri_Neg_adv = df_t_p_t[\"clean_text\"].apply(negative_adverb_count)\n",
        "df_t_p_t[\"Negative adverbs rate\"] = Seri_Neg_adv / Seri_Neg_adv.sum()\n",
        "df_t_p_t.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrLxmk8ce0TV"
      },
      "source": [
        "## Filler words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyyyCWX4c4CN"
      },
      "outputs": [],
      "source": [
        "filler_list = [\"uh\", \"um\", \"hmm\", \"mhm\", \"huh\"]\n",
        "def Filler_word_count(x):\n",
        "    cnt = 0\n",
        "    for word in nlp(x):\n",
        "        if str(word.text) in filler_list:\n",
        "            cnt += 1\n",
        "    return cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOOrj4Av_zsP",
        "outputId": "a378f0fd-d866-471f-db70-c6c59cc5163d"
      },
      "outputs": [],
      "source": [
        "df_t_p_t[\"Filler words\"] = df_t_p_t[\"clean_text\"].apply(Filler_word_count)\n",
        "df_t_p_t.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgJp0gKWdJpV"
      },
      "source": [
        "## Action Verbs rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2GbfetE2LqX"
      },
      "outputs": [],
      "source": [
        "df_action_verb = pd.read_excel(path + \"Data/Action_verbs/action_verbs.xlsx\", header=None)\n",
        "action_verb = df_action_verb.astype(str).to_numpy().reshape(-1)\n",
        "action_verb = np.unique(action_verb)\n",
        "action_verb = np.delete(action_verb, np.where(action_verb=='nan')[0][0])\n",
        "def action_verb_count(x):\n",
        "    cnt = 0\n",
        "    for word in nlp(x):\n",
        "        if str(word.lemma_) in action_verb:\n",
        "            cnt += 1\n",
        "    return cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NL5Z_yqL_pkk",
        "outputId": "bbc397ae-9710-4e3a-b4ad-10189565e919"
      },
      "outputs": [],
      "source": [
        "df_t_p_t[\"action_verb\"] = df_t_p_t[\"clean_text\"].apply(action_verb_count)\n",
        "df_t_p_t.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXgW21TVfASC"
      },
      "source": [
        "## Lexical frequency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dub7aNJic39R",
        "outputId": "711c3352-38e5-407c-91d5-52c3277f5606"
      },
      "outputs": [],
      "source": [
        "real_pos_list = ['VERB', 'PROPN', 'NOUN', 'ADV', 'ADJ']\n",
        "sum_real = []\n",
        "for real_pos in real_pos_list:\n",
        "    sum_real.append(df_t_p_t[real_pos])\n",
        "df_t_p_t[\"Lexical frequency\"] = np.log2(sum(sum_real))\n",
        "df_t_p_t.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJ6FDrWHfPLM"
      },
      "source": [
        "## Definite articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03il-92Fe_FA"
      },
      "outputs": [],
      "source": [
        "def the_count(x):\n",
        "    cnt = 0\n",
        "    for word in nlp(x):\n",
        "        if str(word.text) == \"the\":\n",
        "            cnt += 1\n",
        "    return cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaBd5mcjArRj"
      },
      "outputs": [],
      "source": [
        "df_t_p_t[\"Definite articles\"] = df_t_p_t[\"clean_text\"].apply(the_count) / df_t_p_t.loc[:, \"ADJ\":\"VERB\"].drop(\"PUNCT\", axis=1).sum(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyYQgfjffZ0S"
      },
      "source": [
        "## Indefinites articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cfgiWyXe-_X"
      },
      "outputs": [],
      "source": [
        "a_an_list = [\"a\", \"an\"]\n",
        "def a_an_count(x):\n",
        "    cnt = 0\n",
        "    for word in nlp(x):\n",
        "        if str(word.text) in a_an_list:\n",
        "            cnt += 1\n",
        "    return cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WjpbiaMB8Ak",
        "outputId": "90b60e58-66ad-42f4-9d14-4adb5704450b"
      },
      "outputs": [],
      "source": [
        "df_t_p_t[\"Indefinites articles\"] = df_t_p_t[\"clean_text\"].apply(a_an_count) / df_t_p_t.loc[:, \"ADJ\":\"VERB\"].drop(\"PUNCT\", axis=1).sum(axis=1)\n",
        "df_t_p_t.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vk4kI_gIfm1t"
      },
      "source": [
        "## Pronouns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wkwu9LAlfcjw"
      },
      "outputs": [],
      "source": [
        "df_t_p_t[\"Pronouns\"] = df_t_p_t[\"PRON\"] / df_t_p_t.loc[:, \"ADJ\":\"VERB\"].drop(\"PUNCT\", axis=1).sum(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJFlYW9yfo31"
      },
      "source": [
        "## Nouns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2SKEbyGfceW"
      },
      "outputs": [],
      "source": [
        "df_t_p_t[\"Nouns\"] = df_t_p_t[\"NOUN\"] / df_t_p_t.loc[:, \"ADJ\":\"VERB\"].drop(\"PUNCT\", axis=1).sum(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InfBMA_gfvLK"
      },
      "source": [
        "## Verbs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-VXYkRvfcYk"
      },
      "outputs": [],
      "source": [
        "df_t_p_t[\"Verbs\"] = df_t_p_t[\"VERB\"] / df_t_p_t.loc[:, \"ADJ\":\"VERB\"].drop(\"PUNCT\", axis=1).sum(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ7oVRlafz_n"
      },
      "source": [
        "## Determiners"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgxU51b8fcTS"
      },
      "outputs": [],
      "source": [
        "df_t_p_t[\"Determiners\"] = df_t_p_t[\"DET\"] / df_t_p_t.loc[:, \"ADJ\":\"VERB\"].drop(\"PUNCT\", axis=1).sum(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-P47Leef7WJ"
      },
      "source": [
        "## Content words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkHWNbHvf9Gx"
      },
      "outputs": [],
      "source": [
        "def stop_word_spacy(x):\n",
        "    cnt = 0\n",
        "    for word in nlp(x):\n",
        "        if word.is_stop == False:\n",
        "            cnt += 1\n",
        "    return cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NvN50Y-LCkwz"
      },
      "outputs": [],
      "source": [
        "df_t_p_t[\"Content words\"] = df_t_p_t[\"clean_text\"].apply(stop_word_spacy) / df_t_p_t.loc[:, \"ADJ\":\"VERB\"].drop(\"PUNCT\", axis=1).sum(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeiozvwIhW-K"
      },
      "source": [
        "## Consecutive repeated clauses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgK1W5Wvfewx"
      },
      "outputs": [],
      "source": [
        "def find_repeated_clauses(text):\n",
        "    match = re.findall(r'((\\b.+?\\b)(?:\\s\\2)+)', text)\n",
        "    repeated_clauses = [(m[1], int((len(m[0]) + 1) / (len(m[1]) + 1))) for m in match]\n",
        "    count_list = [count for repeated_clause, count in repeated_clauses]\n",
        "    return sum(count_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMs9ox3Oj4tD",
        "outputId": "c2aeeead-1fda-48c0-cb44-ba6faf83489b"
      },
      "outputs": [],
      "source": [
        "df_t_p_t[\"Consecutive repeated clauses\"] = df_t_p_t['clean_text'].apply(find_repeated_clauses)\n",
        "df_t_p_t.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKiliDDvc1hy"
      },
      "source": [
        "## Lexical Richness: Type-Token Ratio,  and"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sGu_OVmENWO",
        "outputId": "90eac392-ccd9-4642-ae36-fd63b00ed647"
      },
      "outputs": [],
      "source": [
        "# type-token ratio (TTR)\n",
        "# root type-token ratio (RTTR)\n",
        "# corrected type-token ratio (CTTR)\n",
        "# mean segmental type-token ratio (MSTTR)\n",
        "# moving average type-token ratio (MATTR)\n",
        "df_t_p_t['TTR'] = df_t_p_t['clean_text'].apply(lambda x:LexicalRichness(x).ttr)\n",
        "df_t_p_t['RTTR'] = df_t_p_t['clean_text'].apply(lambda x:LexicalRichness(x).rttr)\n",
        "df_t_p_t['CTTR'] = df_t_p_t['clean_text'].apply(lambda x:LexicalRichness(x).cttr)\n",
        "df_t_p_t.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6kH1tZ44vGe"
      },
      "source": [
        "## Total number of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZL8V2jV4wC6"
      },
      "outputs": [],
      "source": [
        "df_t_p_t['Word count'] = df_t_p_t['clean_text'].apply(lambda x:LexicalRichness(x).words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMIRtKvz4wvl"
      },
      "source": [
        "## Total number of unique words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIXgLQiM5KiU"
      },
      "outputs": [],
      "source": [
        "df_t_p_t['Unique Word count'] = df_t_p_t['clean_text'].apply(lambda x:LexicalRichness(x).terms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCwH-EY141Nf"
      },
      "source": [
        "## Ratio of total number of unique words to total number of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwRTkMOR7QbE"
      },
      "outputs": [],
      "source": [
        "df_t_p_t['Ratio unique word count to total word count'] = df_t_p_t['Unique Word count'] / df_t_p_t['Word count']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zHM2pt11_-o"
      },
      "source": [
        "## W - Brunet’s Index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7xsdApsGVaD"
      },
      "outputs": [],
      "source": [
        "df_t_p_t['Brunet’s Index'] = df_t_p_t['Word count'] ** (df_t_p_t['Unique Word count'] ** (-0.165))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GJUmDpd2Bqk"
      },
      "source": [
        "## R - Honor’s Statistic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MCpTKIw2I01"
      },
      "outputs": [],
      "source": [
        "def honore(text):\n",
        "    try:\n",
        "        words = [word.strip(punctuation) for word in text.lower().split()]\n",
        "        N = len(words)\n",
        "        types = set(words)\n",
        "        V = len(types)\n",
        "        V1 = sum(1 for w in words if words.count(w) == 1)\n",
        "        H = 100 * math.log(N) / (1 - V1/V)\n",
        "    except ZeroDivisionError:\n",
        "        H = np.nan\n",
        "    return H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPJejyDKWlxn"
      },
      "outputs": [],
      "source": [
        "df_t_p_t['Honor’s Statistic'] = df_t_p_t['clean_text'].apply(honore)\n",
        "df_t_p_t['Honor’s Statistic'] = df_t_p_t['Honor’s Statistic'].fillna(df_t_p_t['Honor’s Statistic'].max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCm6hmdg2g-b"
      },
      "source": [
        "## Measure of Textual Lexical Diversity (MTLD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2tinSuo2n0L"
      },
      "outputs": [],
      "source": [
        "df_t_p_t['Measure of Textual Lexical Diversity'] = df_t_p_t['clean_text'].apply(lambda x:LexicalRichness(x).mtld(threshold=0.72))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5tzAJ9M2nlx"
      },
      "source": [
        "## Hypergeometric distribution diversity (HD-D) measure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Va4gl9vE2ndE",
        "outputId": "58bd5266-7950-4760-9eb6-a068a9c83889"
      },
      "outputs": [],
      "source": [
        "df_t_p_t['Hypergeometric Distribution Diversity'] = df_t_p_t['clean_text'].apply(lambda x:LexicalRichness(x).hdd(draws=1))\n",
        "df_t_p_t.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1N513wwhRyh"
      },
      "source": [
        "# Finding clauses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wXl2sxtS3912"
      },
      "outputs": [],
      "source": [
        "# -------------------- REPLACEMENT: high-precision, order-preserving clause splitter --------------------\n",
        "BOUNDARY_PUNCT = {\",\", \";\", \"—\", \"–\", \":\"}\n",
        "COORD_WORDS    = {\"and\",\"but\",\"or\",\"so\"}\n",
        "SUBORD_WORDS   = {\"because\",\"although\",\"though\",\"while\",\"when\",\"if\",\"that\",\"who\",\"which\"}\n",
        "REL_WORDS      = {\"that\",\"who\",\"which\"}\n",
        "\n",
        "MIN_TOK = 4\n",
        "VERB_LOOKAHEAD = 6\n",
        "\n",
        "def _is_finite_verb(tok):\n",
        "    if tok.pos_ in (\"VERB\", \"AUX\"):\n",
        "        if tok.tag_ in (\"VBD\",\"VBP\",\"VBZ\",\"MD\"):\n",
        "            return True\n",
        "        m = tok.morph\n",
        "        if \"VerbForm=Fin\" in m or \"Tense=Past\" in m or \"Tense=Pres\" in m:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def _has_finite(span):\n",
        "    return any(_is_finite_verb(t) for t in span)\n",
        "\n",
        "def _verb_within(toks, start, k):\n",
        "    end = min(len(toks), start + k)\n",
        "    return any(_is_finite_verb(t) for t in toks[start:end])\n",
        "\n",
        "def _is_boundary_token(t):\n",
        "    if t.text in BOUNDARY_PUNCT:\n",
        "        return True\n",
        "    if t.pos_ == \"CCONJ\" or t.lemma_.lower() in COORD_WORDS:\n",
        "        return True\n",
        "    if t.lemma_.lower() in SUBORD_WORDS:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def find_clauses_in_order(text, nlp=None, keep_cc=True):\n",
        "    \"\"\"\n",
        "    Order-preserving clause splitter with correct handling of end-of-sentence markers\n",
        "    (e.g., 'because' at EOS stays with the left clause) and contractions.\n",
        "    \"\"\"\n",
        "    if nlp is None:\n",
        "        nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\",\"textcat\",\"lemmatizer\"])\n",
        "\n",
        "    doc = nlp(text)\n",
        "    clauses = []\n",
        "\n",
        "    for sent in doc.sents:\n",
        "        toks = list(sent)\n",
        "        start = 0\n",
        "        has_finite = False\n",
        "        spans = []\n",
        "\n",
        "        i = 0\n",
        "        while i < len(toks):\n",
        "            t = toks[i]\n",
        "            if _is_finite_verb(t):\n",
        "                has_finite = True\n",
        "\n",
        "            commit = False\n",
        "            reason = None\n",
        "\n",
        "            # A) explicit boundary tokens\n",
        "            if _is_boundary_token(t):\n",
        "                lem = t.lemma_.lower()\n",
        "                if lem in COORD_WORDS:\n",
        "                    if has_finite and _verb_within(toks, i+1, VERB_LOOKAHEAD) and (i - start) >= (MIN_TOK - 1):\n",
        "                        commit = True; reason = \"boundary\"\n",
        "                elif lem in SUBORD_WORDS:\n",
        "                    if has_finite and (i - start) >= (MIN_TOK - 1):\n",
        "                        commit = True; reason = \"boundary\"\n",
        "                elif t.text in BOUNDARY_PUNCT:\n",
        "                    if has_finite and (i - start) >= (MIN_TOK - 2):\n",
        "                        commit = True; reason = \"boundary\"\n",
        "\n",
        "            # B) new subject begins a finite clause: split BEFORE the subject (fixes \"she's\" cases)\n",
        "            if (not commit) and t.dep_ in {\"nsubj\",\"nsubjpass\",\"expl\"}:\n",
        "                head = t.head\n",
        "                if head.i > t.i and _is_finite_verb(head) and has_finite and (i - start) >= (MIN_TOK - 1):\n",
        "                    commit = True; reason = \"new_subject\"\n",
        "\n",
        "            # C) end of sentence always commits\n",
        "            if i == len(toks) - 1:\n",
        "                commit = True\n",
        "                if reason is None:\n",
        "                    reason = \"eos\"\n",
        "\n",
        "            if commit:\n",
        "                marker_lemma = toks[i].lemma_.lower()\n",
        "\n",
        "                # Decide span end:\n",
        "                if reason == \"new_subject\":\n",
        "                    end = i                                # cut BEFORE subject\n",
        "                elif reason == \"eos\":\n",
        "                    end = i + 1                           # include final token (e.g., 'because') with LEFT\n",
        "                elif reason == \"boundary\":\n",
        "                    # Keep subordinators/relatives with RIGHT clause when there is one\n",
        "                    if marker_lemma in (REL_WORDS | SUBORD_WORDS):\n",
        "                        end = i                           # cut BEFORE marker\n",
        "                    else:\n",
        "                        end = i + 1                       # include CC/punct with LEFT\n",
        "                else:\n",
        "                    end = i + 1\n",
        "\n",
        "                if end > start:\n",
        "                    span = sent[start:end]\n",
        "                    stext = re.sub(r\"\\s+\", \" \", span.text.strip())\n",
        "                    if not keep_cc:\n",
        "                        stext = re.sub(r\"^(?:and|but|or|so)\\s+\", \"\", sext, flags=re.I)  # noqa: F821 (typo fix below)\n",
        "                    if stext:\n",
        "                        spans.append([start, end, stext])\n",
        "\n",
        "                # Decide next start:\n",
        "                if reason == \"new_subject\":\n",
        "                    start = i                              # next clause starts at the subject\n",
        "                elif reason == \"boundary\" and marker_lemma in (REL_WORDS | SUBORD_WORDS):\n",
        "                    start = i                              # right clause begins with the marker (that/who/because)\n",
        "                else:\n",
        "                    start = i + 1\n",
        "\n",
        "                has_finite = False\n",
        "\n",
        "            i += 1\n",
        "\n",
        "        # Merge tiny/non-finite spans forward or backward\n",
        "        merged = []\n",
        "        for j, (a, b, stext) in enumerate(spans):\n",
        "            span = sent[a:b]\n",
        "            too_short = (len(span) < MIN_TOK)\n",
        "            no_finite = not _has_finite(span)\n",
        "            if (too_short or no_finite) and (j < len(spans) - 1):\n",
        "                na, nb, _ = spans[j+1]\n",
        "                spans[j+1] = [a, nb, re.sub(r\"\\s+\", \" \", sent[a:nb].text.strip())]\n",
        "            elif (too_short or no_finite) and merged:\n",
        "                pa, pb, _ = merged[-1]\n",
        "                merged[-1] = [pa, b, re.sub(r\"\\s+\", \" \", sent[pa:b].text.strip())]\n",
        "            else:\n",
        "                merged.append([a, b, stext])\n",
        "\n",
        "        # Safety: ensure full coverage (no dropped tail)\n",
        "        if merged:\n",
        "            last_a, last_b, _ = merged[-1]\n",
        "            if last_b < len(toks):\n",
        "                merged[-1] = [last_a, len(toks), re.sub(r\"\\s+\", \" \", sent[last_a:len(toks)].text.strip())]\n",
        "        elif len(toks) > 0:\n",
        "            merged.append([0, len(toks), sent[0:len(toks)].text.strip()])\n",
        "\n",
        "        # Emit\n",
        "        for a, b, stext in merged:\n",
        "            s = stext.strip()\n",
        "            if s:\n",
        "                clauses.append(s)\n",
        "\n",
        "    return clauses\n",
        "\n",
        "\n",
        "def correct_start_and_end_of_sentnece(text):\n",
        "    if (text[-2:] == ' s') or (text[-2:] == ' d') or (text[-2:] == ' m'):\n",
        "        text = text[:-2] + text[-1]\n",
        "    elif (text[-3:] == ' nt') or (text[-3:] == ' re') or (text[-3:] == ' ve') or (text[-3:] == ' ll'):\n",
        "        text = text[:-3] + text[-2:]\n",
        "\n",
        "    if text!='' and text!=\"\" and text!=None:\n",
        "        if text[0]==\"'\":\n",
        "            text = text[1:]\n",
        "        if text[-1]==\"'\":\n",
        "            text = text[:-1]\n",
        "    return text.strip()\n",
        "\n",
        "def delete_punctuation(text, dictionary={\",\":\"\", \".\":\"\", \"?\":\"\", \"!\":\"\"}):\n",
        "    for key, value in dictionary.items():\n",
        "        # print(text)\n",
        "        text = text.replace(key, value)\n",
        "        corr_text = correct_start_and_end_of_sentnece(text.strip())\n",
        "    return corr_text\n",
        "\n",
        "def Finding_clauses_list(text):\n",
        "    en = spacy.load('en_core_web_sm')\n",
        "    doc = en(text)\n",
        "    #deplacy.render(doc)\n",
        "\n",
        "    # Use improved, contiguous clause splitter\n",
        "    raw_clauses = find_clauses_in_order(text, nlp=en, keep_cc=True)\n",
        "    # Keep original (index, clause) shape\n",
        "    chunks = [(i, c) for i, c in enumerate(raw_clauses)]\n",
        "\n",
        "    chunks = sorted(chunks, key=lambda x: x[0])\n",
        "    dictionary_clause = {\"gon what na\":\"gonna what\", \"Got ta\":\"Gotta\", \"Gon na\":\"Gonna\", \"got ta\":\"gotta\", \"gon na\":\"gonna\",\n",
        "                         \" nt \":\"nt \", \" re \": \"re \", \" m \":\"m \", \" ve \":\"ve \", \" d \":\"d \", \" ll \":\"ll \",\n",
        "                         \" 've\":\"'ve\", \" n't\":\"n't\", \" 'm\":\"'m\", \" 's\":\"'s\", \" s \":\"s \", \" 're\":\"'re\", \" 'll\":\"'ll\", \" 'd\":\"'d\",\n",
        "                         \" ’ve\":\"’ve\", \" n’t\":\"n’t\", \" ’m\":\"’m\", \" ’s\":\"’s\", \" ’re\":\"’re\", \" ’ll\":\"’ll\", \" ’d\":\"’d\",\n",
        "                         \",\":\"\", \".\":\"\", \"?\":\"\", \"!\":\"\", \"  \":\" \", 'am I gonna what':'what am I gonna', 'gon that na':'that gonna'}\n",
        "    clauses_list = [delete_punctuation(clause, dictionary_clause) for ii, clause in chunks]\n",
        "    # for clause in clauses_list: #ali\n",
        "    #     print(clause)           #ali\n",
        "    return clauses_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYy35ByIKDBS"
      },
      "outputs": [],
      "source": [
        "def similarity_score_between_text(df, df_t_p_t, file_name, postfix_col='without stop word'):\n",
        "    similarity_scores = []\n",
        "    if df.shape[0] <= 1:\n",
        "        df_t_p_t.loc[file_name, 'Mean similarity' + ' ' + postfix_col] = 0\n",
        "        df_t_p_t.loc[file_name, 'Std similarity' + ' ' + postfix_col] = 0\n",
        "        df_t_p_t.loc[file_name, 'Proportion zero similarity' + ' ' + postfix_col] = 0\n",
        "    else:\n",
        "        for i in range(df.shape[0]):\n",
        "            for j in range(i+1, df.shape[0]):\n",
        "                text_1, text_2 = df.loc[i, 'lemma'], df.loc[j, 'lemma']\n",
        "                # Convert the texts into TF-IDF vectors\n",
        "                vectorizer = TfidfVectorizer()\n",
        "                try:\n",
        "                    vectors = vectorizer.fit_transform([text_1, text_2])\n",
        "                except ValueError:\n",
        "                    continue\n",
        "                # Calculate the cosine similarity between the vectors\n",
        "                similarity = cosine_similarity(vectors)\n",
        "                similarity_scores.append(similarity[0, 1])\n",
        "        df_t_p_t.loc[file_name, 'Mean similarity' + ' ' + postfix_col] = np.mean(similarity_scores)\n",
        "        df_t_p_t.loc[file_name, 'Std similarity' + ' ' + postfix_col] = np.std(similarity_scores)\n",
        "        similarity_scores = np.array(similarity_scores)\n",
        "        df_t_p_t.loc[file_name, 'Proportion zero similarity' + ' ' + postfix_col] = len(np.where(similarity_scores==0.0)[0])/len(similarity_scores)\n",
        "    return df_t_p_t\n",
        "\n",
        "def matching_clauses_to_dataframe(df, clauses_list):\n",
        "    # print(\"All clause:\", clauses_list) #ali\n",
        "    # print(\"clause_list:\\n\")\n",
        "    text_remove_punc = ' '.join(df['content_remove_punc'].tolist())\n",
        "    # print(\"Main text:\", text_remove_punc)\n",
        "    remain_clause_words_list = []\n",
        "    df['clauses'] = np.nan\n",
        "    for i, clause in enumerate(clauses_list):\n",
        "        # print(i, clause)\n",
        "        clause_words = clause.split()\n",
        "        if clause in text_remove_punc and clause:\n",
        "            # print(f\"Candidate clause {i+1}:\", clause) #ali\n",
        "            # split_list = text_remove_punc.replace(\"'\", \"\").split(clause) #ali\n",
        "            split_list = text_remove_punc.split(clause)\n",
        "            # for repetitive clauses or there are this clause in other clauses.\n",
        "            if (len(split_list) > 2):\n",
        "                remain_clause_words_list.append((i, clause_words))\n",
        "                # print('clause_' + str(i+1))\n",
        "                # print('******************************')\n",
        "                continue\n",
        "            text_remove_punc = ''\n",
        "            row_start = len(split_list[0].split())\n",
        "            row_end = len(split_list[0].split()) + len(clause_words) - 1\n",
        "            text_remove_punc += split_list[0] + ' '.join(['Claused_filled']*len(clause_words)) + split_list[1]\n",
        "            df.loc[row_start:row_end, 'clauses'] = 'clause_' + str(i+1)\n",
        "\n",
        "        else:\n",
        "            remain_clause_words_list.append((i, clause_words))\n",
        "            # print(remain_clause_words_list) #ali\n",
        "            # print()\n",
        "            # print('clause_' + str(i+1))\n",
        "        # print('******************************')\n",
        "    # print(remain_clause_words_list)\n",
        "    # display(df)\n",
        "    for ii, clause_words in remain_clause_words_list:\n",
        "        # print(clause_words)\n",
        "        df_isnan = df[df['clauses'].isna()]\n",
        "        df_contain = df_isnan[df_isnan['content_remove_punc'].isin(clause_words)]\n",
        "        # display(df_contain) #ali\n",
        "\n",
        "        # display(df_contain)\n",
        "        # print('Clauses__________________:', ' '.join(clause_words))\n",
        "        # print('All words without clauses:', ' '.join(df_contain['content_remove_punc'].tolist()))\n",
        "        # display(df_contain)\n",
        "        # When reversing on word is happend\n",
        "        if len(clause_words) == df_contain.shape[0]:\n",
        "            df.loc[df_contain.index.tolist(), 'clauses'] = 'clause_' + str(ii+1)\n",
        "            continue\n",
        "        index_list_order = []\n",
        "        for i, word in enumerate(clause_words):\n",
        "\n",
        "            # print(\"word in clause:\", word)\n",
        "            # print('**********************************')\n",
        "\n",
        "            index_list = df_contain[df_contain[\"content_remove_punc\"]==clause_words[i]].index.tolist()\n",
        "            # print(index_list)\n",
        "\n",
        "            if i==0:\n",
        "                # Minus one just to first_index_list should be greater than before_first_index_list\n",
        "                index_list_prev = deepcopy(np.array(index_list)-1)\n",
        "            if i<len(clause_words)-1:\n",
        "                index_list_next = df_contain[df_contain[\"content_remove_punc\"]==clause_words[i+1]].index.tolist()\n",
        "            new_index_list = []\n",
        "            for idx in index_list:\n",
        "                if len(clause_words)==1:\n",
        "                    new_index_list.append(idx)\n",
        "                    break\n",
        "                if i==0:\n",
        "                    if (idx < np.array(index_list_next)).any():\n",
        "                        new_index_list.append(idx)\n",
        "                elif i==len(clause_words)-1:\n",
        "                    if (idx > np.array(index_list_prev)).any():\n",
        "                        new_index_list.append(idx)\n",
        "                else:\n",
        "                    if (idx < np.array(index_list_next)).any() and (idx > np.array(index_list_prev)).any():\n",
        "                        new_index_list.append(idx)\n",
        "            if len(new_index_list)!=0:\n",
        "                index_list_prev = deepcopy(new_index_list)\n",
        "                index_list_order.append(new_index_list)\n",
        "            # added lastly\n",
        "            else:\n",
        "                index_list_order.append(index_list)\n",
        "            # print(\"index_list_order:\", index_list_order) #ali\n",
        "            # print(\"************\") #ali\n",
        "\n",
        "        while True:\n",
        "            len_len_index_list_order = len(np.where(np.array([len(idxs) for idxs in index_list_order])>1)[0])\n",
        "            print(len_len_index_list_order)\n",
        "            # print(index_list_order)\n",
        "            if len_len_index_list_order==0:\n",
        "                break\n",
        "            for i in range(len(index_list_order)):\n",
        "                if len(index_list_order[i])> 1:\n",
        "                    if i==0:\n",
        "                        if len(index_list_order[i+1]) > 1:\n",
        "                            continue\n",
        "                        distance = np.array(index_list_order[i]) - index_list_order[i+1][0]\n",
        "                        index_list_order[i] = [index_list_order[i][np.argmin(distance)]]\n",
        "                    elif i==len(index_list_order)-1:\n",
        "                        distance = np.array(index_list_order[i]) - index_list_order[i-1][0]\n",
        "                        index_list_order[i] = [index_list_order[i][np.argmin(distance)]]\n",
        "                    else:\n",
        "                        if len(index_list_order[i-1])==1:\n",
        "                            distance = np.array(index_list_order[i]) - index_list_order[i-1][0]\n",
        "                            index_list_order[i] = [index_list_order[i][np.argmin(distance)]]\n",
        "                        elif len(index_list_order[i+1])==1:\n",
        "                            distance = np.array(index_list_order[i]) - index_list_order[i+1][0]\n",
        "                            index_list_order[i] = [index_list_order[i][np.argmin(distance)]]\n",
        "\n",
        "        flat_index_list = [i for sublist in index_list_order for i in sublist]  # Flatten the list\n",
        "        # print(flat_index_list)#ali\n",
        "        df.loc[flat_index_list, 'clauses'] = 'clause_' + str(ii + 1)\n",
        "        # display(df)\n",
        "    # for filling one row that have nan value in clause column\n",
        "    df_isnan_final = df[df['clauses'].isna()]\n",
        "    if df_isnan_final.shape[0]==1:\n",
        "        index_nan = df_isnan_final.index.tolist()[0]\n",
        "        clause_num = df.loc[index_nan - 1, 'clauses']\n",
        "        clause_num_split = clause_num.split('_')\n",
        "        df.loc[index_nan, 'clauses'] = clause_num_split[0] + '_' + str(int(clause_num_split[1])-1)\n",
        "    elif df_isnan_final.shape[0]>1:\n",
        "        index_nan = df_isnan_final.index.tolist()\n",
        "        index_nan_first = index_nan[0]\n",
        "        index_nan_last = index_nan[-1]\n",
        "        clause_num_before_first = df.loc[index_nan_first - 1, 'clauses']\n",
        "        clause_num_after_last = df.loc[index_nan_last + 1, 'clauses']\n",
        "        if clause_num_before_first == clause_num_after_last:\n",
        "            df.loc[index_nan, 'clauses'] = clause_num_before_first\n",
        "    return df.copy()\n",
        "\n",
        "def add_space_after_punctuation(text):\n",
        "    pattern = r'([,.?!])(?=\\S)'  # Match a comma, period, or question mark followed by a non-space character\n",
        "    replaced_text = re.sub(pattern, r'\\1 ', text)  # Replace the punctuation with punctuation + space\n",
        "    return replaced_text\n",
        "\n",
        "def compute_clause_silence_pos(df_t_p_t):\n",
        "    for i, idx in enumerate(df_t_p_t.index):\n",
        "        df = pd.DataFrame(columns=['content'])\n",
        "        # Read files\n",
        "        dict_modify = {\"*\":\"\",  \"#\":\"\", '2-2':'Two', '->':'', \"–\":\"\", '-':'', \"’\":\"'\", 'wed':\"we'd\",\"mightve\":\"might've\", \"mayve\":\"may've\", \"nt s \":\"nt \",\n",
        "                       \"@\":\"\", \"$\":\"\", \"%\":\"\", \"^\":\"\", \"&\":\"\", \"/\":\"\", \"|\":\"\", \" s \": \"s \", \" re \":\"re \", \" m \":\"m \", \" d \": \"d \", \" ve \":\"ve \", \" ll \":\"ll \",\n",
        "                       ' junʌŋˈjʌnzﬨt':'', '.,':'', ',.':'', '.?':'', '?.':'', \"o'clock\":\"oclock\", \"„\":\"\", ',':'',\n",
        "                       '\"':'', \"\\n\":\"\", ' ...':'', '....':'', '...':'', '..':'', '…':'',  '—':'', \"“\":\"\", \"”\":\"\", \"‘\":\"\", \"’\":\"\", \"  \":\" \", \"   \":\" \", \"''\":\"\", '\"\"':'',\n",
        "                       \"(\":\"\", \")\":\"\", \"[\":\"\", \"]\":\"\", \":\":\"\", \";\":\"\", \"' \":\" \", \"'  \":\" \", \" '\":\" \", \"in'\":\"in\", \",'\":\",\", \".'\":\".\", \"?'\":\"?\", \"!'\":\"!\",\n",
        "                       \" . \":\" \", \" ? \":\" \", \" ! \":\" \", \" , \": \" \"}\n",
        "\n",
        "        df_t_p_t.loc[idx, 'text'] = \" \".join(w.strip() for w in df_t_p_t.loc[idx, 'text'].split())\n",
        "\n",
        "        for key, value in dict_modify.items():\n",
        "            text_value = df_t_p_t.loc[idx, 'text']\n",
        "            if isinstance(text_value, pd.Series):\n",
        "                text_value = text_value.iloc[0]  # Extract single value\n",
        "            df_t_p_t.loc[idx, 'text'] = text_value.strip(string.punctuation).replace(key, value).strip()\n",
        "\n",
        "        df_t_p_t.loc[idx, 'text'] = \" \".join(w.strip() for w in df_t_p_t.loc[idx, 'text'].split())\n",
        "        # print(df_t_p_t.loc[idx, 'text']) #ali\n",
        "\n",
        "\n",
        "        text_value = df_t_p_t.loc[idx, 'text']\n",
        "        if isinstance(text_value, pd.Series):\n",
        "            text_value = text_value.iloc[0]  # Extract single value\n",
        "\n",
        "        # Replace the punctuation with punctuation(.,!?) + space\n",
        "        # print(text_value) #ali\n",
        "        text_preprocessed = add_space_after_punctuation(text_value)\n",
        "        # Correct the end of sentence\n",
        "        text_preprocessed = correct_start_and_end_of_sentnece(text_preprocessed)\n",
        "        df['content'] = text_preprocessed.split(' ')\n",
        "        # display(df)\n",
        "        # Get name of files\n",
        "        file_name = idx\n",
        "        print(f\"{i} - {file_name}:\\n\")\n",
        "\n",
        "        # # Silence time\n",
        "        # df['silence_time'] = (df['start_time'] - df['end_time'].shift(1, fill_value=0)).abs()\n",
        "        # Finding and matching clauses\n",
        "        df['content'] = df['content'].astype(str)\n",
        "        text = ' '.join(df['content'].tolist())\n",
        "\n",
        "        clauses_list = Finding_clauses_list(text)\n",
        "        # print(\"clauses_list:\")  #ali\n",
        "        # print(clauses_list)     #ali\n",
        "        # print(\"Initial clause list:\", clauses_list) #ali\n",
        "        df['content_remove_punc'] = df['content'].apply(delete_punctuation)\n",
        "        # display(df) #ali\n",
        "        # try:\n",
        "        df = matching_clauses_to_dataframe(df, clauses_list)\n",
        "        # display(df) #ali\n",
        "        # Finding POS, Lemma, Stop word, Dependency\n",
        "        nlp = en_core_web_sm.load()\n",
        "        # \"nlp\" Objectis used to create documents with linguistic annotations.\n",
        "        text_remove_punc = ' '.join(df['content_remove_punc'].tolist())\n",
        "        row = 0\n",
        "        # iii = 0\n",
        "        for token in nlp(text_remove_punc):\n",
        "            # print(\"{} : {} : {}\".format(iii, token.text, type(token.text)))\n",
        "            # iii +=1\n",
        "            # yasaman modify\n",
        "            # if (row!=0 and len(df.loc[row-1, 'pos'].split())==1) and (df.loc[row-1, 'content_remove_punc']=='gonna' or df.loc[row-1, 'content_remove_punc']=='gotta') and (\"'s\" in token.text):\n",
        "            if (row!=0 and len(df.loc[row-1, 'pos'].split())==1) and (str(df.loc[row-1, 'content_remove_punc']).lower()=='gonna' or str(df.loc[row-1, 'content_remove_punc']).lower()=='gotta' or str(df.loc[row-1, 'content_remove_punc']).lower()=='cannot')\\\n",
        "                or (\"'\" in token.text) or (token.text=='s') or (token.text=='nt') or (token.text=='re') or (token.text=='m') or (token.text=='d') or (token.text=='ll') or (token.text=='ve'):\n",
        "                # print(df[row-1])\n",
        "                df.loc[row-1, 'pos'] = df.loc[row-1, 'pos'] + ' ' + token.pos_\n",
        "                df.loc[row-1, 'lemma'] = df.loc[row-1, 'lemma'] + ' ' + token.lemma_\n",
        "                df.loc[row-1, 'dependency'] = df.loc[row-1, 'dependency'] + ' ' + token.dep_\n",
        "                continue\n",
        "            else:\n",
        "                df.loc[row, ['pos', 'stop_word', 'lemma', 'dependency']] = \\\n",
        "                                      {'pos':token.pos_, 'stop_word':token.is_stop, 'lemma':token.lemma_, 'dependency':token.dep_}\n",
        "            row += 1\n",
        "        # except:\n",
        "        #     print('This index has problem:', idx)\n",
        "        #     continue\n",
        "        # Save dataframe\n",
        "        # df.to_excel(path_file, index=False)\n",
        "\n",
        "        # # Compute silent clause for each file\n",
        "        # within_clauses_silence_sum = df.groupby('clauses', as_index=False, sort=False)['silence_time'].mean()['silence_time'].sum()\n",
        "        # df_t_p_t.loc[file_name, 'Sum of mean silence time per word within clauses'] = within_clauses_silence_sum\n",
        "        # initial_clauses_silence_sum = df.groupby('clauses', as_index=False, sort=False)['silence_time'].first()['silence_time'].sum()\n",
        "        # df_t_p_t.loc[file_name, 'Sum of mean silence time initial clauses'] = initial_clauses_silence_sum\n",
        "        # # 'NOUN'\n",
        "        # within_clauses_silence_of_nouns = df[df['pos']=='NOUN'].groupby('clauses', as_index=False, sort=False)['silence_time'].mean()['silence_time'].sum()\n",
        "        # df_t_p_t.loc[file_name, 'Sum of mean silence time for NOUN within clauses'] = within_clauses_silence_of_nouns\n",
        "        # # 'VERB'\n",
        "        # within_clauses_silence_of_verbs = df[df['pos']=='VERB'].groupby('clauses', as_index=False, sort=False)['silence_time'].mean()['silence_time'].sum()\n",
        "        # df_t_p_t.loc[file_name, 'Sum of mean silence time for VERB within clauses'] = within_clauses_silence_of_verbs\n",
        "        # # 'ADJ', 'ADV'\n",
        "        # within_clauses_silence_of_adjs_advs = df[df['pos'].isin(['ADJ', 'ADV'])].groupby('clauses', as_index=False, sort=False)['silence_time'].mean()['silence_time'].sum()\n",
        "        # df_t_p_t.loc[file_name, 'Sum of mean silence time for ADJ ADV within clauses'] = within_clauses_silence_of_adjs_advs\n",
        "        # Compute similarity between clauses\n",
        "        ## Without stop words\n",
        "        df_not_stop = df[df['stop_word']==False]\n",
        "        df_not_stop_clause = df_not_stop[['clauses', 'lemma']].groupby('clauses', as_index=False, sort=False).agg(lambda x:x.str.cat(sep=' '))\n",
        "        df_t_p_t = similarity_score_between_text(df_not_stop_clause, df_t_p_t, file_name, postfix_col='without stop word')\n",
        "        ## With stop words\n",
        "        df_clause = df[['clauses', 'lemma']].groupby('clauses', as_index=False, sort=False).agg(lambda x:x.str.cat(sep=' '))\n",
        "        df_t_p_t = similarity_score_between_text(df_clause, df_t_p_t, file_name, postfix_col='with stop word')\n",
        "        display(df[df['clauses'].isna()])\n",
        "        # display(df)\n",
        "        # print('************************\\n')\n",
        "    return df, df_t_p_t.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hX8hwENIsitI",
        "outputId": "c2c6be2a-a3e1-4d18-db40-1533ddc81de4"
      },
      "outputs": [],
      "source": [
        "df_lasr_par, df_t_p_t_final = compute_clause_silence_pos(df_t_p_t.iloc[:, :])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbgWnrXjphq6"
      },
      "source": [
        "# Save Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "HuQ8SkRpQYsa",
        "outputId": "651de6b8-417b-434f-a726-89a10634f86c"
      },
      "outputs": [],
      "source": [
        "df_t_p_t_final.head(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NV0GW486B_Dg"
      },
      "outputs": [],
      "source": [
        "if 'label' in df_t_p_t_final.columns:\n",
        "    label_name = 'label'\n",
        "elif 'Label' in df_t_p_t_final.columns:\n",
        "    label_name = 'Label'\n",
        "df_lingustic = pd.concat([df_t_p_t_final[[label_name]], df_t_p_t_final.loc[:, \"Content Density\":\"Proportion zero similarity with stop word\"]], axis=1)\n",
        "df_lingustic.to_excel(DATA_PATH + './Handcrafted_fr_train_df_story_recall_cinderella.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8HCFeQyRecF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "qVEIV0UE5-E_"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
