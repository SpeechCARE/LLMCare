# Component 4 â€“ Multimodal LLMs (Qwen & Phi)

## Overview

This component explores fine-tuning and evaluating **state-of-the-art multimodal models** for audioâ€“text tasks:

* **Qwen 2.5 Omni**

  * â€œThinker Talkerâ€ architecture handling text, audio, image, and video.
  * Supports real-time speech responses.
  * Fine-tuning via Hugging Face checkpoints and LLaMA-Factory.

* **Phi-4-Multimodal**

  * Microsoftâ€™s multimodal model unifying speech, vision, and language encoders.
  * Supports **128K token context**.
  * Used in open-weight form for domain-specific fine-tuning.

We evaluated both **Qwen** and **Phi-4** in **zero-shot** and **fine-tuned** settings.

---

## ğŸ“‚ Project Structure

```
Component 4 â€“ Multimodal LLMs
â”œâ”€â”€ Phi4/
â”‚   â”œâ”€â”€ finetune.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ run.sh
â”‚   â”œâ”€â”€ test.py
â”‚   â””â”€â”€ test.sh
â”‚
â””â”€â”€ qwen/
    â”œâ”€â”€ create_dataset_json.py
    â”œâ”€â”€ test_audio_classification_2label.py
    â”œâ”€â”€ test_audio_classification_multiclass.py
    â”œâ”€â”€ test_requirements.txt
    â””â”€â”€ train.yaml
```

---

## Qwen

### Setup

```bash
cd qwen
git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git
cd LLaMA-Factory
pip install -e ".[torch,metrics]" --no-build-isolation
```

### ğŸ“‚ Dataset Preparation

Convert your dataset into JSON format:

```bash
python qwen/create_dataset_json.py
```

### Training

Train using the provided YAML config:

```bash
llamafactory-cli train qwen/train.yaml
```

### Inference

Fist install dependencies:

```bash
pip install -r ../qwen/test_requirements.txt   # test dependencies
```

Then, run inference for audio classification:

```bash
python qwen/test_audio_classification.py
```

---

## Phi-4

### Setup

```bash
cd Phi4
pip install -r requirements.txt
```

### Training

Run fine-tuning with the shell script:

```bash
bash finetune.sh
```

### Inference

Run inference with:

```bash
bash test.sh
```

or directly:

```bash
python test.py
```
